{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question 5.ipynb",
      "provenance": [],
      "mount_file_id": "1XQrQb2L3TTrqae2F47PxfKYFjhunHPHW",
      "authorship_tag": "ABX9TyOefrc1M44Kk8NL5NhOLvI9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanePai/Neural_Networks-Assignment-2/blob/main/Question_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngIvFf9vVach"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXGRa0cdLLZS",
        "outputId": "86fa8edc-b9cd-4690-f29a-5b063a6c515b"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/NNFL/Assignment 2\"\n",
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/104/Assignment 2\n",
            "assignment2.pdf  class2_images\tclass_label.mat  data5.mat     input.mat\n",
            "class1_images\t class3_images\tdata55.xlsx\t input_a2.mat  label.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDdt4QF3KiDG"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from sklearn.preprocessing import normalize\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix as cm"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyS9AvMsLHMb"
      },
      "source": [
        "f = loadmat('data5.mat')\n",
        "D = f['x']\n",
        "np.random.shuffle(D)\n",
        "\n",
        "\n",
        "def init_data(data):\n",
        "    X = np.array(data[ : , :-1], dtype = float)\n",
        "    y = np.array(data[ : , -1], dtype = int)\n",
        "    X = (X - X.mean(axis = 0))/X.std(axis = 0)\n",
        "    return X, y\n",
        "\n",
        "X, y = init_data(D)\n",
        "X_train, y_train = X[ :int(0.7 * len(X))], y[ :int(0.7 * len(X))]\n",
        "X_val, y_val = X[ int(0.7 * len(X)): ], y[ int(0.7 * len(X)): ]\n",
        "\n",
        "alpha = 0.5"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPpJxGfeLcyk"
      },
      "source": [
        "def sigmoid(x, derivative=False):\n",
        "        if (derivative == True):\n",
        "            return x * (1 - x)\n",
        "        return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIJZyQmnOm0r"
      },
      "source": [
        "class NeuralNetwork(object):\n",
        "    def __init__(self, sizes):\n",
        "        \n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.W = {}\n",
        "        self.a = {}\n",
        "        self.b = {}\n",
        "        \n",
        "        #Initialize Weights\n",
        "        for i in range(1, self.num_layers):\n",
        "            self.W[i] = np.random.randn(self.sizes[i-1], self.sizes[i])\n",
        "            \n",
        "        #Initialize biases\n",
        "        for i in range(1, self.num_layers):\n",
        "            self.b[i] = np.random.randn(self.sizes[i], 1)\n",
        "        \n",
        "        #Initialize activations\n",
        "        for i in range(1, self.num_layers):\n",
        "            self.a[i] = np.zeros([self.sizes[i], 1])\n",
        "        \n",
        "    #Forward pass\n",
        "    def forward_pass(self, X):\n",
        "        \n",
        "        self.a[0] = X\n",
        "        \n",
        "        for i in range(1, self.num_layers):\n",
        "            self.a[i] = sigmoid(np.dot(self.W[i].T, self.a[i-1]) + self.b[i])\n",
        "\n",
        "        return self.a[self.num_layers-1] \n",
        "    \n",
        "    #Backward pass/ Weight Update\n",
        "    def backward_pass(self, X, Y, output):\n",
        "        \n",
        "        self.d = {}\n",
        "        self.d_output = (Y - output) * sigmoid(output, derivative=True)\n",
        "        self.d[self.num_layers-1] = self.d_output\n",
        "        \n",
        "        #Derivatives of the layers\n",
        "        for i in range(self.num_layers-1, 1, -1):\n",
        "            self.d[i-1] = np.dot(self.W[i], self.d[i]) * sigmoid(self.a[i-1], derivative=True)\n",
        "        \n",
        "        #Updating weights\n",
        "        for i in range(1, self.num_layers-1):\n",
        "            self.W[i] += alpha * np.dot(self.a[i-1], self.d[i].T)\n",
        "            \n",
        "        #Updating biases\n",
        "        for i in range(1, self.num_layers-1):\n",
        "            self.b[i] += alpha * self.d[i]\n",
        "\n",
        "    def train(self, X, Y):\n",
        "        X = np.reshape(X, (len(X), 1))\n",
        "        output = self.forward_pass(X)\n",
        "        self.backward_pass(X, Y, output)\n",
        "    \n",
        "    def get_W(self):\n",
        "        return self.W\n",
        "    \n",
        "    def load_W(self, W):\n",
        "        self.W = W\n",
        "    \n",
        "    def get_a(self, x):\n",
        "        x = np.reshape(x, (len(x), 1))\n",
        "        self.forward_pass(x)\n",
        "        return self.a\n",
        "    \n",
        "    def load_a(self, a):\n",
        "        self.a = a"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rUc_fl4V0c-"
      },
      "source": [
        "#Cost function\n",
        "def calc_cost(NN,x ,y):\n",
        "    \n",
        "    cost = 0\n",
        "    for i in range(len(x)):\n",
        "        x_ = np.reshape(x[i], (len(x[i]), 1))\n",
        "        cost += 0.5 / len(x) * np.sum((y[i] - NN.forward_pass(x_)) ** 2)\n",
        "    \n",
        "    return cost\n",
        "\n",
        "#Network initialization\n",
        "autoencoder1 = NeuralNetwork([72, 60, 72])\n",
        "autoencoder2 = NeuralNetwork([60,40,60])\n",
        "autoencoder3 = NeuralNetwork([40, 30, 40])\n",
        "NN = NeuralNetwork([72,60,40,30, 1])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGSGsaPUV5j0",
        "outputId": "ea355773-2aa8-4ba6-dc76-697e5f52e3f0"
      },
      "source": [
        "#Autoencoder 1 pretraining\n",
        "print(\"Autoencoder 1\")\n",
        "for i in range(200):\n",
        "    for j, row in enumerate(X_train):\n",
        "        row = np.reshape(row, (72,1))\n",
        "        autoencoder1.train(row, row)\n",
        "        \n",
        "    cost = calc_cost(autoencoder1, X_train, X_train)\n",
        "    if (i+1)%100 == 0:\n",
        "        print(f\"Epoch: {i+1}, Cost: {cost}\")\n",
        "    \n",
        "#Scores computation for autoencoder 1\n",
        "autoencoder2_input = []\n",
        "\n",
        "for row in X_train:\n",
        "    autoencoder2_input.append(autoencoder1.get_a(row)[1])\n",
        "\n",
        "autoencoder2_input = np.array(autoencoder2_input)\n",
        "\n",
        "\n",
        "#Autoencoder 2 pretraining\n",
        "print(\"Autoencoder 2\")\n",
        "for i in range(200):\n",
        "    for j, row in enumerate(autoencoder2_input):\n",
        "        row = np.reshape(row, (60,1))\n",
        "        autoencoder2.train(row, row)\n",
        "        \n",
        "    cost = calc_cost(autoencoder2, autoencoder2_input, autoencoder2_input)\n",
        "    if (i+1)%100 == 0:\n",
        "        print(f\"Epoch: {i+1}, Cost: {cost}\")\n",
        "\n",
        "\n",
        "#Scores computation for autoencoder 2\n",
        "autoencoder3_input = []\n",
        "\n",
        "for row in autoencoder2_input:\n",
        "    autoencoder3_input.append(autoencoder2.get_a(row)[1])\n",
        "\n",
        "autoencoder3_input = np.array(autoencoder3_input)\n",
        "\n",
        "#Autoencoder 3 pretraining\n",
        "print(\"Autoencoder 3\")\n",
        "for i in range(200):\n",
        "    for j, row in enumerate(autoencoder3_input):\n",
        "        row = np.reshape(row, (40,1))\n",
        "        autoencoder3.train(row, row)\n",
        "        \n",
        "    cost = calc_cost(autoencoder3, autoencoder3_input, autoencoder3_input)\n",
        "    if (1+i)%100 == 0:\n",
        "        print(f\"Epoch: {i+1}, Cost: {cost}\")\n",
        "\n",
        "#Final network weight initialization\n",
        "W1 = autoencoder1.get_W()[1]\n",
        "W2 = autoencoder2.get_W()[1]\n",
        "W3 = autoencoder3.get_W()[1]\n",
        "W_final = {}\n",
        "W_final[1] = W1\n",
        "W_final[2] = W2\n",
        "W_final[3] = W3\n",
        "W_final[4] = np.random.randn(30, 1)\n",
        "NN.load_W(W_final)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Autoencoder 1\n",
            "Epoch: 100, Cost: 3216.9559953970356\n",
            "Epoch: 200, Cost: 3209.1462642445617\n",
            "Autoencoder 2\n",
            "Epoch: 100, Cost: 4.78892496094285\n",
            "Epoch: 200, Cost: 4.772383897367545\n",
            "Autoencoder 3\n",
            "Epoch: 100, Cost: 2.484828192250361\n",
            "Epoch: 200, Cost: 2.4779127632846967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQwa-BfpOipi",
        "outputId": "40a6674f-ead2-4f53-8d90-48d030317a4a"
      },
      "source": [
        "#Training\n",
        "for i in range(500):\n",
        "    if (i+1)%100==0:\n",
        "        print(\"Epoch: \", i+1)\n",
        "\n",
        "    for j in range(len(X_train)):\n",
        "        NN.train(X_train[j], y_train[j])\n",
        "\n",
        "y_pred = []\n",
        "for i in range(len(X_val)):\n",
        "\n",
        "    x = np.reshape(X_val[i], (len(X_val[i]), 1))\n",
        "    x = NN.forward_pass(x)\n",
        "    p = 0 if x[0] < 0.5 else 1\n",
        "    y_pred.append(p)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  100\n",
            "Epoch:  200\n",
            "Epoch:  300\n",
            "Epoch:  400\n",
            "Epoch:  500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNcqQk-iqqao",
        "outputId": "1ee92769-d39b-486e-aeba-b71a1d52d7c3"
      },
      "source": [
        "confmat = cm(y_val, y_pred)\n",
        "\n",
        "Accuracy = (confmat[0][0]+confmat[1][1])/len(y_pred)\n",
        "Sensitivity = (confmat[1][1])/(confmat[1][0] + confmat[1][1])\n",
        "Specificity = (confmat[0][0])/(confmat[0][0] + confmat[0][1])\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confmat)\n",
        "print(\"\\n\")\n",
        "print(f\"Accuracy: {Accuracy*100}%\\nSensitivity: {Sensitivity}\\nSpecificity: {Specificity}\\n\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[264  49]\n",
            " [ 46 286]]\n",
            "\n",
            "\n",
            "Accuracy: 85.27131782945736%\n",
            "Sensitivity: 0.8614457831325302\n",
            "Specificity: 0.8434504792332268\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}